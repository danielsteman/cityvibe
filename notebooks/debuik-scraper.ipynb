{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "71a7d3fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All dependencies are available\n",
      "‚úÖ Playwright async API imported successfully\n",
      "‚úÖ Successfully imported cityvibe_core models\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Add workspace packages to Python path for notebook execution\n",
    "notebook_dir = Path(os.getcwd())\n",
    "if notebook_dir.name == \"notebooks\":\n",
    "    project_root = notebook_dir.parent\n",
    "else:\n",
    "    project_root = notebook_dir if (notebook_dir / \"packages\" / \"cityvibe-core\").exists() else notebook_dir.parent\n",
    "\n",
    "core_path = project_root / \"packages\" / \"cityvibe-core\" / \"src\"\n",
    "common_path = project_root / \"packages\" / \"cityvibe-common\" / \"src\"\n",
    "\n",
    "if core_path.exists():\n",
    "    sys.path.insert(0, str(core_path))\n",
    "if common_path.exists():\n",
    "    sys.path.insert(0, str(common_path))\n",
    "\n",
    "# Install missing dependencies if needed\n",
    "dependencies_installed = False\n",
    "\n",
    "try:\n",
    "    import sqlalchemy\n",
    "    import playwright\n",
    "    print(\"‚úÖ All dependencies are available\")\n",
    "except ImportError as e:\n",
    "    missing = str(e).split()[-1].replace(\"'\", \"\")\n",
    "    print(f\"‚ö†Ô∏è {missing} not found. Installing dependencies...\")\n",
    "    import subprocess\n",
    "    try:\n",
    "        subprocess.check_call([\n",
    "            sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\",\n",
    "            \"sqlalchemy>=2.0.0\", \"sqlmodel>=0.0.14\", \"asyncpg>=0.29.0\", \n",
    "            \"pydantic>=2.0.0\", \"alembic>=1.13.0\", \"psycopg2-binary>=2.9.0\",\n",
    "            \"playwright>=1.40.0\"\n",
    "        ], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "        print(\"‚úÖ Dependencies installed successfully!\")\n",
    "        print(\"‚ö†Ô∏è Installing Playwright browsers (this may take a moment)...\")\n",
    "        subprocess.check_call([\n",
    "            sys.executable, \"-m\", \"playwright\", \"install\", \"chromium\"\n",
    "        ], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "        print(\"‚úÖ Playwright browsers installed!\")\n",
    "        # Force reload after installation\n",
    "        import importlib\n",
    "        importlib.invalidate_caches()\n",
    "        dependencies_installed = True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to install dependencies: {e}\")\n",
    "        print(\"\\nüí° Please install manually with:\")\n",
    "        print(f\"   {sys.executable} -m pip install sqlalchemy>=2.0.0 sqlmodel>=0.0.14 asyncpg>=0.29.0 pydantic>=2.0.0 alembic>=1.13.0 psycopg2-binary>=2.9.0 playwright>=1.40.0\")\n",
    "        print(f\"   {sys.executable} -m playwright install chromium\")\n",
    "        raise\n",
    "\n",
    "import asyncio\n",
    "import time\n",
    "import json\n",
    "import logging\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from decimal import Decimal, InvalidOperation\n",
    "from typing import Any, Optional, AsyncGenerator, Dict, List\n",
    "from uuid import UUID\n",
    "from datetime import datetime\n",
    "import httpx\n",
    "\n",
    "# Import playwright async API (required for notebooks/Jupyter)\n",
    "try:\n",
    "    from playwright.async_api import async_playwright\n",
    "    print(\"‚úÖ Playwright async API imported successfully\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è Playwright not available (optional)\")\n",
    "\n",
    "# Import venue models after dependencies are installed\n",
    "from cityvibe_core.models.venue import Venue, VenueCreate, VenuePublic\n",
    "print(\"‚úÖ Successfully imported cityvibe_core models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0088adbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30403e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_lat_lon(soup: BeautifulSoup) -> tuple[Optional[Decimal], Optional[Decimal]]:\n",
    "    \"\"\"Haalt latitude en longitude uit Google Maps afbeeldingen in de HTML.\"\"\"\n",
    "    url_to_check = None\n",
    "    \n",
    "    # 1. Sidebar map\n",
    "    map_img = soup.select_one(\".locatie-small img\")\n",
    "    if map_img:\n",
    "        src_attr = map_img.get('src')\n",
    "        if src_attr:\n",
    "            url_to_check = str(src_attr)\n",
    "    \n",
    "    # 2. Fallback: Grote map\n",
    "    if not url_to_check:\n",
    "        map_div = soup.select_one(\".locatie-large a\")\n",
    "        if map_div:\n",
    "            style_attr = map_div.get('style')\n",
    "            if style_attr:\n",
    "                url_to_check = str(style_attr)\n",
    "\n",
    "    if url_to_check and isinstance(url_to_check, str):\n",
    "        match = re.search(r'center=([\\d\\.]+),([\\d\\.]+)', url_to_check)\n",
    "        if match:\n",
    "            try:\n",
    "                return Decimal(match.group(1)), Decimal(match.group(2))\n",
    "            except (InvalidOperation, ValueError):\n",
    "                pass\n",
    "    return None, None\n",
    "\n",
    "def extract_opening_hours(soup: BeautifulSoup) -> Dict[str, str]:\n",
    "    \"\"\"Parseert de HTML tabel met openingstijden.\"\"\"\n",
    "    hours = {}\n",
    "    table = soup.select_one(\".openingstijden-tabel\")\n",
    "    \n",
    "    if not table:\n",
    "        return hours\n",
    "\n",
    "    rows = table.select(\".openingstijden-tabel-tr\")\n",
    "    for row in rows:\n",
    "        label_div = row.select_one(\".openingstijden-label div\")\n",
    "        if not label_div:\n",
    "            continue\n",
    "        day = label_div.get_text(strip=True)\n",
    "\n",
    "        data_div = row.select_one(\".openingstijden-data\")\n",
    "        if not data_div:\n",
    "            hours[day] = \"Unknown\"\n",
    "            continue\n",
    "\n",
    "        closed_div = data_div.select_one(\".openingstijden-gesloten\")\n",
    "        if closed_div:\n",
    "            hours[day] = \"Gesloten\"\n",
    "        else:\n",
    "            time_div = data_div.select_one(\".openingstijden-restaurant\")\n",
    "            if time_div:\n",
    "                start = time_div.select_one(\".start\")\n",
    "                end = time_div.select_one(\".einde\")\n",
    "                s_txt = start.get_text(strip=True) if start else \"?\"\n",
    "                e_txt = end.get_text(strip=True) if end else \"?\"\n",
    "                hours[day] = f\"{s_txt} - {e_txt}\"\n",
    "            else:\n",
    "                hours[day] = \"Unknown\"\n",
    "    return hours\n",
    "\n",
    "def extract_venue_features(soup: BeautifulSoup) -> tuple[str, Dict[str, str]]:\n",
    "    \"\"\"Haalt het type zaak en de lijst met kenmerken op.\"\"\"\n",
    "    venue_type = \"Restaurant\"\n",
    "    features = {}\n",
    "    \n",
    "    kenmerken_div = soup.select_one(\".kenmerken .content\")\n",
    "    if kenmerken_div:\n",
    "        for dl in kenmerken_div.find_all(\"dl\"):\n",
    "            dt = dl.find(\"dt\")\n",
    "            dd = dl.find(\"dd\")\n",
    "            if dt and dd:\n",
    "                key = dt.get_text(strip=True)\n",
    "                val = dd.get_text(strip=True)\n",
    "                features[key] = val\n",
    "                if \"Soort zaak\" in key:\n",
    "                    venue_type = val\n",
    "    return venue_type, features\n",
    "\n",
    "def extract_address_info(soup: BeautifulSoup) -> Dict[str, str | None]:\n",
    "    \"\"\"Haalt straat, postcode, stad en naam op.\"\"\"\n",
    "    info = {\"name\": None, \"street\": None, \"zip_code\": None, \"city\": \"Amsterdam\"}\n",
    "    \n",
    "    address_div = soup.select_one(\".address\")\n",
    "    if address_div:\n",
    "        h1 = address_div.find(\"h1\")\n",
    "        if h1:\n",
    "            info[\"name\"] = h1.get_text(strip=True)\n",
    "        \n",
    "        street_span = address_div.select_one(\".street\")\n",
    "        if street_span:\n",
    "            info[\"street\"] = street_span.get_text(strip=True)\n",
    "            \n",
    "        zip_span = address_div.select_one(\".postcode\")\n",
    "        if zip_span:\n",
    "            info[\"zip_code\"] = zip_span.get_text(strip=True)\n",
    "        \n",
    "        city_span = address_div.select_one(\".city\")\n",
    "        if city_span:\n",
    "            info[\"city\"] = city_span.get_text(strip=True)\n",
    "            \n",
    "    return info\n",
    "\n",
    "def parse_venue_html(html_content: str, url: str) -> Optional[VenueCreate]:\n",
    "    \"\"\"\n",
    "    De hoofd parser. Voegt alle bovenstaande functies samen.\n",
    "    Input: Rauwe HTML string en de URL.\n",
    "    Output: Een VenueCreate object of None.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    \n",
    "    # 1. Basis Info\n",
    "    addr_info = extract_address_info(soup)\n",
    "    if not addr_info[\"name\"]:\n",
    "        logging.warning(f\"Parse error: Geen naam gevonden voor {url}\")\n",
    "        return None\n",
    "\n",
    "    # 2. Geo & Features & Uren (Deel-functies aanroepen)\n",
    "    lat, lon = extract_lat_lon(soup)\n",
    "    venue_type, features = extract_venue_features(soup)\n",
    "    opening_hours = extract_opening_hours(soup)\n",
    "\n",
    "    # 3. Externe Links & Plaatjes (Simpel genoeg om hier te houden)\n",
    "    external_website = None\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        txt = a.get_text().lower()\n",
    "        if \"website\" in txt and \"debuik\" not in a['href']:\n",
    "            external_website = a['href']\n",
    "            break\n",
    "    \n",
    "    final_url = external_website if external_website else url\n",
    "\n",
    "    image_url = None\n",
    "    img_tag = soup.select_one(\"img.imgfade-transition\")\n",
    "    if img_tag and img_tag.get(\"src\"):\n",
    "        image_url = img_tag[\"src\"]\n",
    "\n",
    "    # 4. Construct Object\n",
    "    config = {\n",
    "        \"source\": \"debuik.nl\",\n",
    "        \"original_url\": url,\n",
    "        \"scraped_at\": datetime.utcnow().isoformat(),\n",
    "        \"street\": addr_info[\"street\"],\n",
    "        \"zip_code\": addr_info[\"zip_code\"],\n",
    "        \"image_url\": image_url,\n",
    "        \"features\": features,\n",
    "        \"opening_hours\": opening_hours\n",
    "    }\n",
    "\n",
    "    return VenueCreate(\n",
    "        name=addr_info[\"name\"],\n",
    "        website_url=final_url,\n",
    "        city=addr_info[\"city\"],\n",
    "        state=\"Noord-Holland\",\n",
    "        country=\"NL\",\n",
    "        latitude=lat,\n",
    "        longitude=lon,\n",
    "        venue_type=venue_type,\n",
    "        scraper_config=config,\n",
    "        active=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9b1829b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEARCH_URL = \"https://www.debuik.nl/amsterdam/zoek/restaurant\"\n",
    "\n",
    "async def fetch_listing_urls(limit: int = 10) -> List[str]:\n",
    "    \"\"\"Gebruikt Playwright om de zoekpagina te scrollen en URLs te verzamelen.\"\"\"\n",
    "    logging.info(f\"Start Playwright (limit={limit})...\")\n",
    "    unique_urls = []\n",
    "    \n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        page = await browser.new_page()\n",
    "        \n",
    "        try:\n",
    "            await page.goto(SEARCH_URL)\n",
    "            await page.wait_for_selector('a[href*=\"/amsterdam/restaurant/\"]', timeout=15000)\n",
    "            \n",
    "            # Scrollen als we meer nodig hebben dan de initi√´le lading\n",
    "            if limit > 10:\n",
    "                for _ in range(2):\n",
    "                    await page.mouse.wheel(0, 3000)\n",
    "                    await asyncio.sleep(1)\n",
    "            \n",
    "            hrefs = await page.evaluate(\"\"\"() => {\n",
    "                return Array.from(document.querySelectorAll('a[href*=\"/amsterdam/restaurant/\"]'))\n",
    "                    .map(a => a.href)\n",
    "            }\"\"\")\n",
    "            \n",
    "            # Filteren en uniek maken\n",
    "            for href in hrefs:\n",
    "                if \"/amsterdam/restaurant/\" in href:\n",
    "                     unique_urls.append(href)\n",
    "            \n",
    "            unique_urls = list(set(unique_urls))\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Playwright error: {e}\")\n",
    "        finally:\n",
    "            await browser.close()\n",
    "            \n",
    "    return unique_urls[:limit]\n",
    "\n",
    "async def fetch_and_parse_venue(client: httpx.AsyncClient, url: str) -> Optional[VenueCreate]:\n",
    "    \"\"\"Haalt HTML op met HTTPX en stuurt het naar de parser.\"\"\"\n",
    "    try:\n",
    "        response = await client.get(url, timeout=15)\n",
    "        if response.status_code == 200:\n",
    "            # Hier roepen we de 'Pure Functie' aan uit sectie 2\n",
    "            return parse_venue_html(response.text, url)\n",
    "        else:\n",
    "            logging.warning(f\"Status {response.status_code} voor {url}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error scraping {url}: {e}\")\n",
    "    return None\n",
    "\n",
    "async def run_scraper(limit: int = 5) -> List[VenueCreate]:\n",
    "    \"\"\"De 'Main' functie die alles aan elkaar knoopt.\"\"\"\n",
    "    \n",
    "    # 1. Haal URLs op\n",
    "    urls = await fetch_listing_urls(limit)\n",
    "    logging.info(f\"{len(urls)} URLs gevonden. Details ophalen...\")\n",
    "    \n",
    "    results = []\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (compatible; Scraper/1.0)\"}\n",
    "    \n",
    "    # 2. Haal details op\n",
    "    async with httpx.AsyncClient(headers=headers) as client:\n",
    "        for i, url in enumerate(urls, 1):\n",
    "            logging.info(f\"Processing ({i}/{len(urls)}): {url}\")\n",
    "            \n",
    "            venue = await fetch_and_parse_venue(client, url)\n",
    "            if venue:\n",
    "                results.append(venue)\n",
    "            \n",
    "            await asyncio.sleep(0.5) # Rate limiting\n",
    "            \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a278e4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Start Playwright browser voor: https://www.debuik.nl/amsterdam/restaurant/petitbysam\n",
      "‚úÖ HTML opgehaald. Start parser...\n",
      "\n",
      "üéâ SUCCESVOL GESPRAAPT:\n",
      "============================================================\n",
      "Naam:        PetitbySam\n",
      "Type:        Delicatessenzaak\n",
      "Adres:       Vijzelstraat 93, 1017 HA\n",
      "Geo:         52.3633362, 4.8924013\n",
      "Img:         https://www.debuik.nl/fp/zhdlWu6QpalbAUPWOhfU/convert?&w=2660&h=1290&fit=crop\n",
      "Desc:        PetitbySam van is een fijne delicatessenzaak aan de Vijzelstraat in Amsterdam. Eigenaresse Smaita Ra...\n",
      "------------------------------------------------------------\n",
      "Openingstijden:\n",
      "{\n",
      "  \"Maandag\": \"Gesloten\",\n",
      "  \"Dinsdag\": \"08:00 - 18:00\",\n",
      "  \"Woensdag\": \"08:00 - 18:00\",\n",
      "  \"Donderdag\": \"08:00 - 18:00\",\n",
      "  \"Vrijdag\": \"08:00 - 18:00\",\n",
      "  \"Zaterdag\": \"10:00 - 18:00\",\n",
      "  \"Zondag\": \"10:00 - 18:00\"\n",
      "}\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g0/gf5vqxk16zdf5qt4bcxxwqfm0000gn/T/ipykernel_68978/1192453901.py:122: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"scraped_at\": datetime.utcnow().isoformat(),\n"
     ]
    }
   ],
   "source": [
    "def extract_address_info(soup: BeautifulSoup) -> Dict[str, str | None]:\n",
    "    info = {\"name\": None, \"street\": None, \"zip_code\": None, \"city\": \"Amsterdam\"}\n",
    "    \n",
    "    # 1. Probeer de .address div (Server Side Rendered)\n",
    "    address_div = soup.select_one(\"div.address\")\n",
    "    if address_div:\n",
    "        h1 = address_div.find(\"h1\")\n",
    "        if h1: info[\"name\"] = h1.get_text(strip=True)\n",
    "        if address_div.select_one(\".street\"): info[\"street\"] = address_div.select_one(\".street\").get_text(strip=True)\n",
    "        if address_div.select_one(\".postcode\"): info[\"zip_code\"] = address_div.select_one(\".postcode\").get_text(strip=True)\n",
    "        if address_div.select_one(\".city\"): info[\"city\"] = address_div.select_one(\".city\").get_text(strip=True)\n",
    "        \n",
    "    # 2. Fallback: Zoek naar H1 als de div structuur anders is (voor de zekerheid)\n",
    "    if not info[\"name\"]:\n",
    "        h1 = soup.find(\"h1\")\n",
    "        if h1: info[\"name\"] = h1.get_text(strip=True)\n",
    "\n",
    "    return info\n",
    "\n",
    "def extract_description(soup: BeautifulSoup) -> Optional[str]:\n",
    "    intro = soup.select_one(\".introductie\")\n",
    "    if intro: return intro.get_text(separator=\" \", strip=True)\n",
    "    return None\n",
    "\n",
    "def extract_image(soup: BeautifulSoup) -> Optional[str]:\n",
    "    # Slideshow image\n",
    "    img = soup.select_one(\".restaurant-slideshow .restaurant-slide img.imgfade-transition\")\n",
    "    if img and img.get(\"src\"): return img[\"src\"]\n",
    "    # Thumbnail fallback\n",
    "    thumb = soup.select_one(\".thumbnails img\")\n",
    "    if thumb and thumb.get(\"src\"): return thumb[\"src\"]\n",
    "    return None\n",
    "\n",
    "def extract_lat_lon(soup: BeautifulSoup) -> tuple[Optional[Decimal], Optional[Decimal]]:\n",
    "    url_to_check = None\n",
    "    map_img = soup.select_one(\".locatie-small img\")\n",
    "    if map_img: url_to_check = map_img.get('src')\n",
    "    \n",
    "    if not url_to_check:\n",
    "        map_div = soup.select_one(\".locatie-large a\")\n",
    "        if map_div: url_to_check = map_div.get('style')\n",
    "\n",
    "    if url_to_check:\n",
    "        match = re.search(r'center=([\\d\\.]+),([\\d\\.]+)', str(url_to_check))\n",
    "        if match:\n",
    "            try:\n",
    "                return Decimal(match.group(1)), Decimal(match.group(2))\n",
    "            except: pass\n",
    "    return None, None\n",
    "\n",
    "def extract_opening_hours(soup: BeautifulSoup) -> Dict[str, str]:\n",
    "    hours = {}\n",
    "    table = soup.select_one(\".openingstijden-tabel\")\n",
    "    if not table: return hours\n",
    "\n",
    "    for row in table.select(\".openingstijden-tabel-tr\"):\n",
    "        day_div = row.select_one(\".openingstijden-label div\")\n",
    "        if not day_div: continue\n",
    "        day = day_div.get_text(strip=True)\n",
    "\n",
    "        data_div = row.select_one(\".openingstijden-data\")\n",
    "        if not data_div: continue\n",
    "\n",
    "        if data_div.select_one(\".openingstijden-gesloten\"):\n",
    "            hours[day] = \"Gesloten\"\n",
    "        else:\n",
    "            time_div = data_div.select_one(\".openingstijden-restaurant\")\n",
    "            if time_div:\n",
    "                s = time_div.select_one(\".start\").get_text(strip=True)\n",
    "                e = time_div.select_one(\".einde\").get_text(strip=True)\n",
    "                hours[day] = f\"{s} - {e}\"\n",
    "            else:\n",
    "                hours[day] = \"Unknown\"\n",
    "    return hours\n",
    "\n",
    "def extract_venue_features(soup: BeautifulSoup) -> tuple[str, Dict[str, str]]:\n",
    "    venue_type = \"Restaurant\"\n",
    "    features = {}\n",
    "    kenmerken_div = soup.select_one(\".kenmerken .content\")\n",
    "    if kenmerken_div:\n",
    "        for dl in kenmerken_div.find_all(\"dl\"):\n",
    "            dt = dl.find(\"dt\")\n",
    "            dd = dl.find(\"dd\")\n",
    "            if dt and dd:\n",
    "                k = dt.get_text(strip=True)\n",
    "                v = dd.get_text(strip=True)\n",
    "                features[k] = v\n",
    "                if \"Soort zaak\" in k: venue_type = v\n",
    "    return venue_type, features\n",
    "\n",
    "def parse_venue_html(html_content: str, url: str) -> Optional[VenueCreate]:\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    \n",
    "    # 1. Basis Info\n",
    "    addr_info = extract_address_info(soup)\n",
    "    if not addr_info[\"name\"]:\n",
    "        # DEBUG: Print de titel als het mislukt\n",
    "        print(f\"‚ö†Ô∏è  Parse Error. Pagina Titel: {soup.title.string if soup.title else 'Geen titel'}\")\n",
    "        return None\n",
    "\n",
    "    # 2. Extracties\n",
    "    lat, lon = extract_lat_lon(soup)\n",
    "    venue_type, features = extract_venue_features(soup)\n",
    "    opening_hours = extract_opening_hours(soup)\n",
    "    description = extract_description(soup)\n",
    "    image_url = extract_image(soup)\n",
    "\n",
    "    # 3. Website (Fallback op eigen URL als geen externe link)\n",
    "    external_website = None\n",
    "    sidebar = soup.select_one(\".restaurant-contactvlak\")\n",
    "    if sidebar:\n",
    "        for a in sidebar.find_all(\"a\", href=True):\n",
    "            if \"website\" in a.get_text().lower() and \"debuik.nl\" not in a['href']:\n",
    "                external_website = a['href']\n",
    "                break\n",
    "    \n",
    "    final_url = external_website if external_website else url\n",
    "\n",
    "    config = {\n",
    "        \"source\": \"debuik.nl\",\n",
    "        \"original_url\": url,\n",
    "        \"scraped_at\": datetime.utcnow().isoformat(),\n",
    "        \"street\": addr_info[\"street\"],\n",
    "        \"zip_code\": addr_info[\"zip_code\"],\n",
    "        \"description\": description,\n",
    "        \"image_url\": image_url,\n",
    "        \"features\": features,\n",
    "        \"opening_hours\": opening_hours\n",
    "    }\n",
    "\n",
    "    return VenueCreate(\n",
    "        name=addr_info[\"name\"],\n",
    "        website_url=final_url,\n",
    "        city=addr_info[\"city\"],\n",
    "        state=\"Noord-Holland\",\n",
    "        country=\"NL\",\n",
    "        latitude=lat,\n",
    "        longitude=lon,\n",
    "        venue_type=venue_type,\n",
    "        scraper_config=config,\n",
    "        active=True\n",
    "    )\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. TEST MET PLAYWRIGHT (Browser simulatie)\n",
    "# ==============================================================================\n",
    "\n",
    "async def test_single_url_with_browser(target_url: str):\n",
    "    print(f\"üöÄ Start Playwright browser voor: {target_url}\")\n",
    "    \n",
    "    async with async_playwright() as p:\n",
    "        # Start browser (headless=True zie je niks, zet False om te kijken)\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        context = await browser.new_context(\n",
    "            user_agent=\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "        )\n",
    "        page = await context.new_page()\n",
    "        \n",
    "        try:\n",
    "            # Ga naar pagina\n",
    "            await page.goto(target_url, timeout=30000)\n",
    "            \n",
    "            # Wacht even tot de .address div er is (belangrijk!)\n",
    "            try:\n",
    "                await page.wait_for_selector(\"div.address\", timeout=5000)\n",
    "            except:\n",
    "                print(\"‚ö†Ô∏è  Timeout: Kon .address div niet vinden op de pagina.\")\n",
    "            \n",
    "            # Haal de volledige HTML op\n",
    "            html_content = await page.content()\n",
    "            \n",
    "            print(\"‚úÖ HTML opgehaald. Start parser...\")\n",
    "            venue = parse_venue_html(html_content, target_url)\n",
    "            \n",
    "            if venue:\n",
    "                print(\"\\nüéâ SUCCESVOL GESPRAAPT:\")\n",
    "                print(\"=\"*60)\n",
    "                print(f\"Naam:        {venue.name}\")\n",
    "                print(f\"Type:        {venue.venue_type}\")\n",
    "                print(f\"Adres:       {venue.scraper_config['street']}, {venue.scraper_config['zip_code']}\")\n",
    "                print(f\"Geo:         {venue.latitude}, {venue.longitude}\")\n",
    "                print(f\"Img:         {venue.scraper_config['image_url']}\")\n",
    "                if venue.scraper_config.get('description'):\n",
    "                    print(f\"Desc:        {venue.scraper_config['description'][:100]}...\")\n",
    "                print(\"-\" * 60)\n",
    "                print(\"Openingstijden:\")\n",
    "                print(json.dumps(venue.scraper_config['opening_hours'], indent=2))\n",
    "                print(\"=\"*60)\n",
    "            else:\n",
    "                print(\"‚ùå Parsen mislukt.\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Browser error: {e}\")\n",
    "        finally:\n",
    "            await browser.close()\n",
    "\n",
    "# Start de test\n",
    "url = \"https://www.debuik.nl/amsterdam/restaurant/petitbysam\"\n",
    "await test_single_url_with_browser(url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
